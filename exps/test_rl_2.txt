python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 1 --seed 0
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 1 --seed 1
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 2 --seed 0
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 2 --seed 1
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 4 --seed 0
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 4 --seed 1
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 8 --seed 0
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 8 --seed 1
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 16 --seed 0
python train_RL_agent.py --policy TD3 --relabel_type ground_truth --opt_steps_per_env_step 16 --seed 1
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 1 --seed 0
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 1 --seed 1
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 2 --seed 0
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 2 --seed 1
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 4 --seed 0
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 4 --seed 1
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 8 --seed 0
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 8 --seed 1
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 16 --seed 0
python train_RL_agent.py --policy TD3 --opt_steps_per_env_step 16 --seed 1
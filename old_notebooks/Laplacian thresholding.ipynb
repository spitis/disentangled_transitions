{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses graph laplacian constraint to retrieve the block diagonal matrix, given a noisy version of said matrix\n",
    "\n",
    "Works 100% on seeds 0-4. \n",
    "\n",
    "Gradient-based / differentiable version of the k-BDMS projection in Section 4.2 of http://openaccess.thecvf.com/content_cvpr_2014/papers/Feng_Robust_Subspace_Segmentation_2014_CVPR_paper.pdf.\n",
    "\n",
    "Requires k (connected components) to be known, but maybe this can be relaxed somehow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.linalg as slinalg\n",
    "from scipy.sparse import csgraph\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets make a noisy block diagonal matrix (note: entries should be positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [4, 3, 2]\n",
    "dims = sum(splits)\n",
    "noise = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52, 0.71, 0.63, 0.63, 0.01, 0.09, 0.01, 0.04, 0.01],\n",
       "       [0.41, 0.75, 0.55, 0.52, 0.05, 0.02, 0.06, 0.05, 0.05],\n",
       "       [0.87, 0.95, 0.91, 0.82, 0.  , 0.03, 0.02, 0.04, 0.05],\n",
       "       [0.92, 0.57, 0.4 , 0.31, 0.01, 0.05, 0.05, 0.03, 0.  ],\n",
       "       [0.05, 0.09, 0.01, 0.  , 0.41, 0.37, 0.74, 0.07, 0.05],\n",
       "       [0.01, 0.03, 0.09, 0.08, 0.46, 0.63, 0.5 , 0.05, 0.01],\n",
       "       [0.05, 0.1 , 0.  , 0.01, 0.44, 0.26, 0.17, 0.02, 0.  ],\n",
       "       [0.  , 0.05, 0.01, 0.02, 0.02, 0.1 , 0.07, 0.99, 0.45],\n",
       "       [0.09, 0.02, 0.07, 0.04, 0.  , 0.06, 0.03, 0.06, 0.33]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = []\n",
    "for split in splits:\n",
    "  components.append(np.ones((split, split)))\n",
    "block_diag_mask = slinalg.block_diag(*components)\n",
    "base_matrix = np.random.uniform(size=(dims, dims)) * block_diag_mask\n",
    "noise_matrix = np.random.randn(dims, dims) * noise\n",
    "noisy_matrix = np.abs(base_matrix + noise_matrix).round(2)\n",
    "noisy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe that randomly thresholding isn't super effective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52, 0.71, 0.63, 0.63, 0.  , 0.09, 0.  , 0.  , 0.  ],\n",
       "       [0.41, 0.75, 0.55, 0.52, 0.05, 0.  , 0.06, 0.05, 0.05],\n",
       "       [0.87, 0.95, 0.91, 0.82, 0.  , 0.  , 0.  , 0.  , 0.05],\n",
       "       [0.92, 0.57, 0.4 , 0.31, 0.  , 0.05, 0.05, 0.  , 0.  ],\n",
       "       [0.05, 0.09, 0.  , 0.  , 0.41, 0.37, 0.74, 0.07, 0.05],\n",
       "       [0.  , 0.  , 0.09, 0.08, 0.46, 0.63, 0.5 , 0.05, 0.  ],\n",
       "       [0.05, 0.1 , 0.  , 0.  , 0.44, 0.26, 0.17, 0.  , 0.  ],\n",
       "       [0.  , 0.05, 0.  , 0.  , 0.  , 0.1 , 0.07, 0.99, 0.45],\n",
       "       [0.09, 0.  , 0.07, 0.  , 0.  , 0.06, 0.  , 0.06, 0.33]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholded = noisy_matrix.copy()\n",
    "thresholded[thresholded < 0.05] = 0.\n",
    "thresholded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get the Laplacian matrix L first. \n",
    "\n",
    "If A in R^{NxN}+ (positive reals) has K connected components, the rank of L is N-K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.4 , -0.71, -0.63, -0.63, -0.01, -0.09, -0.01, -0.04, -0.01],\n",
       "       [-0.41,  2.52, -0.55, -0.52, -0.05, -0.02, -0.06, -0.05, -0.05],\n",
       "       [-0.87, -0.95,  1.76, -0.82, -0.  , -0.03, -0.02, -0.04, -0.05],\n",
       "       [-0.92, -0.57, -0.4 ,  2.12, -0.01, -0.05, -0.05, -0.03, -0.  ],\n",
       "       [-0.05, -0.09, -0.01, -0.  ,  0.99, -0.37, -0.74, -0.07, -0.05],\n",
       "       [-0.01, -0.03, -0.09, -0.08, -0.46,  0.98, -0.5 , -0.05, -0.01],\n",
       "       [-0.05, -0.1 , -0.  , -0.01, -0.44, -0.26,  1.48, -0.02, -0.  ],\n",
       "       [-0.  , -0.05, -0.01, -0.02, -0.02, -0.1 , -0.07,  0.36, -0.45],\n",
       "       [-0.09, -0.02, -0.07, -0.04, -0.  , -0.06, -0.03, -0.06,  0.62]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def laplacian(A):\n",
    "  \"\"\"\n",
    "  My implementation; w_ij = -w_ij for i != j; w_ii = sum_{j != i} w_ij\n",
    "  A bit slower than Scipy's for numpy arrays. \n",
    "  \n",
    "  Works for both numpy array and torch tensor. \n",
    "  \n",
    "  Note that this will be a differentiable function of A.\n",
    "  Note that Laplacian at most rank n-1. \n",
    "  \"\"\"\n",
    "  eye = torch.eye if torch.is_tensor(A) else np.eye\n",
    "  I = eye(len(A))\n",
    "  return (I - 1) * A + I * ((1-I)*A).sum(0, keepdims=True)\n",
    "\n",
    "assert np.all(laplacian(noisy_matrix) == csgraph.laplacian(noisy_matrix))\n",
    "csgraph.laplacian(noisy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "At = torch.tensor(noisy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = laplacian(At)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're going to approximate $A$ by $\\hat A$ and optimize laplacian of $\\hat A$ to be low rank.\n",
    "\n",
    "Doing this with Pytorch and gradient descent instead of how it is done in the paper http://openaccess.thecvf.com/content_cvpr_2014/papers/Feng_Robust_Subspace_Segmentation_2014_CVPR_paper.pdf (they use quadratic programming solver).\n",
    "\n",
    "First make a function to find a low rank approx using PCA, which we will compute and use as a learning target. \n",
    "(another way might be to parameterize the low rank approximation as a matrix product) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(A, rank):\n",
    "  \"\"\"\n",
    "  Uses PCA to compute a low rank approximation to A.\n",
    "  \"\"\"\n",
    "  assert rank <= len(A)\n",
    "  U, S, V = torch.svd(A)\n",
    "  return torch.mm(torch.mm(U[:,:rank], torch.diag(S[:rank])), V[:,:rank].t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 approximation error: 22.162927314392885\n",
      "Rank 2 approximation error: 12.881264137512689\n",
      "Rank 3 approximation error: 6.377761684976274\n",
      "Rank 4 approximation error: 2.762220613383946\n",
      "Rank 5 approximation error: 0.8777604546228892\n",
      "Rank 6 approximation error: 0.2095245274252015\n",
      "Rank 7 approximation error: 0.09926253369258037\n",
      "Rank 8 approximation error: 6.533879114449026e-28\n"
     ]
    }
   ],
   "source": [
    "# Verify that this works\n",
    "for i in range(1, len(L)):\n",
    "  low_rank_L = low_rank_approx(L, i)\n",
    "  print('Rank {} approximation error: {}'.format(torch.matrix_rank(low_rank_L), torch.sum((low_rank_L - L)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_At_sqrt = torch.sqrt(At.detach())\n",
    "hat_At_sqrt.requires_grad_(True)\n",
    "K = 3 # number of connected components\n",
    "optimizer = torch.optim.Adam([hat_At_sqrt], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "  hat_At = hat_At_sqrt ** 2\n",
    "  hat_L = laplacian(hat_At + hat_At.T)\n",
    "  low_rank_target = low_rank_approx(hat_L, len(hat_L) - K)\n",
    "  loss = F.l1_loss(hat_L, low_rank_target) + F.mse_loss(hat_At, At)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.  , -0.  ,  0.  ,  0.  , -0.01, -0.09, -0.01, -0.04, -0.01],\n",
       "       [-0.  , -0.  , -0.  , -0.  , -0.05, -0.02, -0.06, -0.05, -0.05],\n",
       "       [-0.  , -0.  ,  0.  , -0.  ,  0.  , -0.03, -0.02, -0.04, -0.05],\n",
       "       [-0.  , -0.  ,  0.  , -0.  , -0.01, -0.05, -0.05, -0.03,  0.  ],\n",
       "       [-0.05, -0.09, -0.01,  0.  ,  0.  , -0.  ,  0.  , -0.07, -0.05],\n",
       "       [-0.01, -0.03, -0.09, -0.08,  0.  ,  0.  , -0.  , -0.05, -0.01],\n",
       "       [-0.05, -0.1 ,  0.  , -0.01, -0.  , -0.  ,  0.  , -0.02,  0.  ],\n",
       "       [ 0.  , -0.05, -0.01, -0.02, -0.02, -0.1 , -0.07,  0.  , -0.  ],\n",
       "       [-0.09, -0.02, -0.07, -0.04,  0.  , -0.06, -0.03, -0.  ,  0.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hat_At - At).detach().numpy().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behold the thresholded matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52, 0.71, 0.63, 0.63, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.41, 0.75, 0.55, 0.52, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.87, 0.95, 0.91, 0.82, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.92, 0.57, 0.4 , 0.31, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.41, 0.37, 0.74, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.46, 0.63, 0.5 , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.44, 0.26, 0.17, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.99, 0.45],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.06, 0.33]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hat_At.detach().numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.52, 0.71, 0.63, 0.63, 0.01, 0.09, 0.01, 0.04, 0.01],\n",
       "       [0.41, 0.75, 0.55, 0.52, 0.05, 0.02, 0.06, 0.05, 0.05],\n",
       "       [0.87, 0.95, 0.91, 0.82, 0.  , 0.03, 0.02, 0.04, 0.05],\n",
       "       [0.92, 0.57, 0.4 , 0.31, 0.01, 0.05, 0.05, 0.03, 0.  ],\n",
       "       [0.05, 0.09, 0.01, 0.  , 0.41, 0.37, 0.74, 0.07, 0.05],\n",
       "       [0.01, 0.03, 0.09, 0.08, 0.46, 0.63, 0.5 , 0.05, 0.01],\n",
       "       [0.05, 0.1 , 0.  , 0.01, 0.44, 0.26, 0.17, 0.02, 0.  ],\n",
       "       [0.  , 0.05, 0.01, 0.02, 0.02, 0.1 , 0.07, 0.99, 0.45],\n",
       "       [0.09, 0.02, 0.07, 0.04, 0.  , 0.06, 0.03, 0.06, 0.33]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At.numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

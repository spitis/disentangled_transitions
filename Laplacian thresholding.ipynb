{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook uses graph laplacian constraint to retrieve the block diagonal matrix, given a noisy version of said matrix\n",
    "\n",
    "Works 100% on seeds 0-4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy.linalg as slinalg\n",
    "from scipy.sparse import csgraph\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First lets make a noisy block diagonal matrix (note: entries should be positive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [4, 3, 2]\n",
    "dims = sum(splits)\n",
    "noise = 5e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98, 0.51, 1.11, 0.74, 0.  , 0.06, 0.  , 0.07, 0.01],\n",
       "       [0.46, 0.76, 0.15, 0.84, 0.04, 0.01, 0.07, 0.08, 0.07],\n",
       "       [0.04, 0.94, 0.35, 0.86, 0.04, 0.01, 0.04, 0.06, 0.02],\n",
       "       [0.22, 0.74, 0.38, 0.54, 0.02, 0.02, 0.07, 0.  , 0.03],\n",
       "       [0.04, 0.04, 0.03, 0.  , 0.42, 0.73, 0.73, 0.1 , 0.01],\n",
       "       [0.04, 0.04, 0.08, 0.03, 0.68, 0.9 , 0.38, 0.07, 0.01],\n",
       "       [0.07, 0.02, 0.02, 0.03, 0.85, 0.44, 0.85, 0.06, 0.1 ],\n",
       "       [0.04, 0.02, 0.05, 0.02, 0.01, 0.01, 0.04, 0.75, 0.9 ],\n",
       "       [0.02, 0.03, 0.06, 0.04, 0.04, 0.02, 0.02, 0.76, 0.08]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = []\n",
    "for split in splits:\n",
    "  components.append(np.ones((split, split)))\n",
    "block_diag_mask = slinalg.block_diag(*components)\n",
    "base_matrix = np.random.uniform(size=(dims, dims)) * block_diag_mask\n",
    "noise_matrix = np.random.randn(dims, dims) * noise\n",
    "noisy_matrix = np.abs(base_matrix + noise_matrix).round(2)\n",
    "noisy_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe that randomly thresholding isn't super effective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98, 0.51, 1.11, 0.74, 0.  , 0.06, 0.  , 0.07, 0.  ],\n",
       "       [0.46, 0.76, 0.15, 0.84, 0.  , 0.  , 0.07, 0.08, 0.07],\n",
       "       [0.  , 0.94, 0.35, 0.86, 0.  , 0.  , 0.  , 0.06, 0.  ],\n",
       "       [0.22, 0.74, 0.38, 0.54, 0.  , 0.  , 0.07, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.42, 0.73, 0.73, 0.1 , 0.  ],\n",
       "       [0.  , 0.  , 0.08, 0.  , 0.68, 0.9 , 0.38, 0.07, 0.  ],\n",
       "       [0.07, 0.  , 0.  , 0.  , 0.85, 0.44, 0.85, 0.06, 0.1 ],\n",
       "       [0.  , 0.  , 0.05, 0.  , 0.  , 0.  , 0.  , 0.75, 0.9 ],\n",
       "       [0.  , 0.  , 0.06, 0.  , 0.  , 0.  , 0.  , 0.76, 0.08]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresholded = noisy_matrix.copy()\n",
    "thresholded[thresholded < 0.05] = 0.\n",
    "thresholded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets get the Laplacian matrix L first. \n",
    "\n",
    "If A in R^{NxN}+ (positive reals) has K connected components, the rank of L is N-K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.93, -0.51, -1.11, -0.74, -0.  , -0.06, -0.  , -0.07, -0.01],\n",
       "       [-0.46,  2.34, -0.15, -0.84, -0.04, -0.01, -0.07, -0.08, -0.07],\n",
       "       [-0.04, -0.94,  1.88, -0.86, -0.04, -0.01, -0.04, -0.06, -0.02],\n",
       "       [-0.22, -0.74, -0.38,  2.56, -0.02, -0.02, -0.07, -0.  , -0.03],\n",
       "       [-0.04, -0.04, -0.03, -0.  ,  1.68, -0.73, -0.73, -0.1 , -0.01],\n",
       "       [-0.04, -0.04, -0.08, -0.03, -0.68,  1.3 , -0.38, -0.07, -0.01],\n",
       "       [-0.07, -0.02, -0.02, -0.03, -0.85, -0.44,  1.35, -0.06, -0.1 ],\n",
       "       [-0.04, -0.02, -0.05, -0.02, -0.01, -0.01, -0.04,  1.2 , -0.9 ],\n",
       "       [-0.02, -0.03, -0.06, -0.04, -0.04, -0.02, -0.02, -0.76,  1.15]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def laplacian(A):\n",
    "  \"\"\"\n",
    "  My implementation; w_ij = -w_ij for i != j; w_ii = sum_{j != i} w_ij\n",
    "  A bit slower than Scipy's for numpy arrays. \n",
    "  \n",
    "  Works for both numpy array and torch tensor. \n",
    "  \n",
    "  Note that this will be a differentiable function of A.\n",
    "  Note that Laplacian at most rank n-1. \n",
    "  \"\"\"\n",
    "  eye = torch.eye if torch.is_tensor(A) else np.eye\n",
    "  I = eye(len(A))\n",
    "  return (I - 1) * A + I * ((1-I)*A).sum(0, keepdims=True)\n",
    "\n",
    "assert np.all(laplacian(noisy_matrix) == csgraph.laplacian(noisy_matrix))\n",
    "csgraph.laplacian(noisy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "At = torch.tensor(noisy_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = laplacian(At)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we're going to approximate $A$ by $\\hat A$ and optimize laplacian of $\\hat A$ to be low rank.\n",
    "\n",
    "Doing this with Pytorch and gradient descent instead of how it is done in the paper http://openaccess.thecvf.com/content_cvpr_2014/papers/Feng_Robust_Subspace_Segmentation_2014_CVPR_paper.pdf (they use quadratic programming solver).\n",
    "\n",
    "First make a function to find a low rank approx using PCA, which we will compute and use as a learning target. \n",
    "(another way might be to parameterize the low rank approximation as a matrix product) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_rank_approx(A, rank):\n",
    "  \"\"\"\n",
    "  Uses PCA to compute a low rank approximation to A.\n",
    "  \"\"\"\n",
    "  assert rank <= len(A)\n",
    "  U, S, V = torch.svd(A)\n",
    "  return torch.mm(torch.mm(U[:,:rank], torch.diag(S[:rank])), V[:,:rank].t())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 approximation error: 24.267596936057103\n",
      "Rank 2 approximation error: 17.222321487422356\n",
      "Rank 3 approximation error: 11.323908627841078\n",
      "Rank 4 approximation error: 7.190660808959906\n",
      "Rank 5 approximation error: 3.2489762647701905\n",
      "Rank 6 approximation error: 0.25137874998117804\n",
      "Rank 7 approximation error: 0.08383584308686802\n",
      "Rank 8 approximation error: 7.960309488000459e-29\n"
     ]
    }
   ],
   "source": [
    "# Verify that this works\n",
    "for i in range(1, len(L)):\n",
    "  low_rank_L = low_rank_approx(L, i)\n",
    "  print('Rank {} approximation error: {}'.format(torch.matrix_rank(low_rank_L), torch.sum((low_rank_L - L)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hat_At_sqrt = torch.sqrt(At.detach())\n",
    "hat_At_sqrt.requires_grad_(True)\n",
    "K = 3 # number of connected components\n",
    "optimizer = torch.optim.Adam([hat_At_sqrt], lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "  hat_At = hat_At_sqrt ** 2\n",
    "  hat_L = laplacian(hat_At + hat_At.T)\n",
    "  low_rank_target = low_rank_approx(hat_L, len(hat_L) - K)\n",
    "  loss = F.l1_loss(hat_L, low_rank_target) + F.mse_loss(hat_At, At)\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.  ,  0.  ,  0.  ,  0.  ,  0.  , -0.06,  0.  , -0.07, -0.01],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  , -0.04, -0.01, -0.07, -0.08, -0.07],\n",
       "       [ 0.  ,  0.  ,  0.  ,  0.  , -0.04, -0.01, -0.04, -0.06, -0.02],\n",
       "       [-0.  , -0.  , -0.  ,  0.  , -0.02, -0.02, -0.07,  0.  , -0.03],\n",
       "       [-0.04, -0.04, -0.03,  0.  ,  0.  ,  0.  ,  0.  , -0.1 , -0.01],\n",
       "       [-0.04, -0.04, -0.08, -0.03,  0.  , -0.  , -0.  , -0.07, -0.01],\n",
       "       [-0.07, -0.02, -0.02, -0.03,  0.  , -0.  , -0.  , -0.06, -0.1 ],\n",
       "       [-0.04, -0.02, -0.05, -0.02, -0.01, -0.01, -0.04, -0.  , -0.  ],\n",
       "       [-0.02, -0.03, -0.06, -0.04, -0.04, -0.02, -0.02,  0.  ,  0.  ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(hat_At - At).detach().numpy().round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behold the thresholded matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98, 0.51, 1.11, 0.74, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.46, 0.76, 0.15, 0.84, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.04, 0.94, 0.35, 0.86, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.22, 0.74, 0.38, 0.54, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.42, 0.73, 0.73, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.68, 0.9 , 0.38, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.85, 0.44, 0.85, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.75, 0.9 ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.76, 0.08]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hat_At.detach().numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98, 0.51, 1.11, 0.74, 0.  , 0.06, 0.  , 0.07, 0.01],\n",
       "       [0.46, 0.76, 0.15, 0.84, 0.04, 0.01, 0.07, 0.08, 0.07],\n",
       "       [0.04, 0.94, 0.35, 0.86, 0.04, 0.01, 0.04, 0.06, 0.02],\n",
       "       [0.22, 0.74, 0.38, 0.54, 0.02, 0.02, 0.07, 0.  , 0.03],\n",
       "       [0.04, 0.04, 0.03, 0.  , 0.42, 0.73, 0.73, 0.1 , 0.01],\n",
       "       [0.04, 0.04, 0.08, 0.03, 0.68, 0.9 , 0.38, 0.07, 0.01],\n",
       "       [0.07, 0.02, 0.02, 0.03, 0.85, 0.44, 0.85, 0.06, 0.1 ],\n",
       "       [0.04, 0.02, 0.05, 0.02, 0.01, 0.01, 0.04, 0.75, 0.9 ],\n",
       "       [0.02, 0.03, 0.06, 0.04, 0.04, 0.02, 0.02, 0.76, 0.08]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "At.numpy().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
